{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebda9efd-52a9-4c18-931b-b2d7250ce7f9",
   "metadata": {},
   "source": [
    "This notebook contains rough working and thoughts for the resulting blog post. For the full, more polished post, visit [my website](https://barnabywreford.co.uk/problems/vector_orthogonality_and_llm_superposition/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f94e4-b996-41a8-990b-dd778d0e71ef",
   "metadata": {},
   "source": [
    "## Working and Rough Talking Points\n",
    "\n",
    "Illustration of how the distance between points grows in higher dimensions by looking at all points within 0.5 distance of the centre point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b12e6-17ea-417d-b69b-962a2741db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from itertools import combinations, product\n",
    "\n",
    "def plot_line(ax, r):\n",
    "    # 1D Line [0,1]\n",
    "    ax.plot([0, 1], [0, 0], color='black')\n",
    "    center = 0.5\n",
    "    ax.plot(center, 0, 'ro')\n",
    "\n",
    "    # Highlighted segment within radius r\n",
    "    left = max(center - r, 0)\n",
    "    right = min(center + r, 1)\n",
    "    ax.plot([left, right], [0, 0], color='red', linewidth=5, alpha=0.3)\n",
    "\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('1D Line')\n",
    "\n",
    "def plot_square(ax, r):\n",
    "    # 2D Square [0,1]^2\n",
    "    square = plt.Rectangle((0, 0), 1, 1, fill=None, edgecolor='black')\n",
    "    ax.add_patch(square)\n",
    "    point = (0.5, 0.5)\n",
    "    ax.plot(*point, 'ro')\n",
    "    circle = plt.Circle(point, r, color='red', alpha=0.3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('2D Square')\n",
    "\n",
    "def plot_cube(ax, r):\n",
    "    # 3D Cube [0,1]^3\n",
    "    cube_edges = combinations(np.array(list(product([0, 1], repeat=3))), 2)\n",
    "    for s, e in cube_edges:\n",
    "        if np.sum(np.abs(s - e)) == 1:\n",
    "            ax.plot3D(*zip(s, e), color=\"black\")\n",
    "\n",
    "    point = np.array([0.5, 0.5, 0.5])\n",
    "    ax.scatter(*point, color='red')\n",
    "\n",
    "    # Sphere around point\n",
    "    u, v = np.mgrid[0:2*np.pi:30j, 0:np.pi:20j]\n",
    "    x = r * np.cos(u) * np.sin(v) + point[0]\n",
    "    y = r * np.sin(u) * np.sin(v) + point[1]\n",
    "    z = r * np.cos(v) + point[2]\n",
    "    ax.plot_surface(x, y, z, color='red', alpha=0.3)\n",
    "\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.set_ylim([-0.1, 1.1])\n",
    "    ax.set_zlim([-0.1, 1.1])\n",
    "    ax.view_init(30, 30)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('3D Cube')\n",
    "\n",
    "def plot_all(r=0.2):\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    plot_line(ax1, r)\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    plot_square(ax2, r)\n",
    "\n",
    "    ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "    plot_cube(ax3, r)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_all(r=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d3514-d3d4-41eb-8c7d-54ac5c5be10d",
   "metadata": {},
   "source": [
    "It may seem relatively unintuitive as to why increasing the number of dimensions means random vectors become increasingly likely to be near-orthoganol.\n",
    "\n",
    "One way to think about it is by looking at the mathematical expression for the angle between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad96c85-8de3-4563-ba83-c007843a126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_angle(v1, v2):\n",
    "    cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    radians = np.arccos(np.clip(cos_theta, -1.0, 1.0))  # ensure numerical stability\n",
    "    return np.degrees(radians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60dc972-95dc-49e5-8138-4208b7be912f",
   "metadata": {},
   "source": [
    "When the two vectors are nearly orthagonal, this means $\\cos{\\theta}$ is nearly 0. If we assume that as we vary our vectors over different dimensions we keep the magnitude of those vectors the same, this must mean we are looking to our dot product $a \\cdot b$ to achieve this near 0 behaviour. If we were to choose our vector directions randomly, then this dot product is a summation of random numbers.\n",
    "\n",
    "To more intuitively imagine a summation of random numbers, imagine rolling a dice with values \\[-3,-2,1,1,2,3\\], rolling it many times and taking the sum. If you roll it twice, the minimum and maximum values you can get is is \\[-6,6\\], and you are reasonably likely to get close to either extreme. However, if you roll the dice 100 times, the min and max values you can get is \\[-300, 300\\], however you are likely to get a score close to 0 as your rolls cancel out. The variance proportionally reducing as the number of trials increases is a fundamental of statistics.\n",
    "\n",
    "Let's visualise this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46fc9d-2e48-4bcb-82de-d45f7f8c8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, max_bins=19, title=None, symmetric=True, center=0):\n",
    "    data = np.asarray(data)\n",
    "    unique_values = np.unique(data)\n",
    "    num_unique = len(unique_values)\n",
    "\n",
    "    if num_unique <= max_bins:\n",
    "        # Use unique-value binning with small padding\n",
    "        bins = np.concatenate([\n",
    "            [unique_values[0] - 0.0001],\n",
    "            (unique_values[:-1] + unique_values[1:]) / 2,\n",
    "            [unique_values[-1] + 0.0001]\n",
    "        ])\n",
    "    else:\n",
    "        if symmetric:\n",
    "            # Symmetric binning around specified center\n",
    "            max_offset = np.max(np.abs(data - center))\n",
    "            bin_min = center - max_offset\n",
    "            bin_max = center + max_offset\n",
    "            bins = np.linspace(bin_min, bin_max, max_bins + 1)\n",
    "        else:\n",
    "            bins = np.linspace(np.min(data), np.max(data), max_bins + 1)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data, bins=bins, density=True, edgecolor='black')\n",
    "    plt.title(title or f'Probability Distribution (n={len(data)})')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70873d42-0d84-4fdb-9a77-42e7963fd082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_dice_sums(n, trials=100_000):\n",
    "    dice = np.array([-3, -2, -1, 1, 2, 3])\n",
    "    results = np.random.choice(dice, size=(trials, n))\n",
    "    sums = np.sum(results, axis=1)\n",
    "    plot_histogram(sums, title=f'Dice Sums n={n}')\n",
    "\n",
    "simulate_dice_sums(n=2)\n",
    "simulate_dice_sums(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74198bb6-7c58-4205-bebe-a412621956ff",
   "metadata": {},
   "source": [
    "In our original equation, we divide by the magnitude of $a$ and $b$, in our dice example this is analogous to dividing by number of rolls. Therefore, the full analogy is to roll our dice n times, take the sum, then divide by n. I hope it is intuitive that as the number of rolls increases, we are likely to get a final score that is closer to 0.\n",
    "\n",
    "However, let's visualise this to confirm the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce6e3d-f152-4c0b-a2a8-2d8b9d9e02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_weighted_dice_sums(n, trials=100_000):\n",
    "    dice = np.array([-3, -2, -1, 1, 2, 3])\n",
    "    results = np.random.choice(dice, size=(trials, n))\n",
    "    sums = np.sum(results, axis=1) / (3*n)\n",
    "    plot_histogram(sums, title=f'Weighted Dice Sums n={n}')\n",
    "\n",
    "simulate_weighted_dice_sums(n=2)\n",
    "simulate_weighted_dice_sums(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ab99d-ba48-457a-b1e3-b2894b22acd7",
   "metadata": {},
   "source": [
    "In the same way as with the dice, when calculating the angle between our vectors, the number of dimensions increases we are more likely to get near-orthogonal vectors.\n",
    "\n",
    "Let's re-run the experiment, this time using random vectors instead of random dice rolls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3ca77-79f5-4ee8-b665-7c04f7be982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_angles(trials, d, memory_MB=1000):\n",
    "    # randn returns a 64 bit float\n",
    "    # so memory used = 2 * batch_size * d * 8\n",
    "    batch_size = memory_MB*1024*1024 // (16*d)\n",
    "    \n",
    "    angles = np.empty(trials)\n",
    "    for start in range(0, trials, batch_size):\n",
    "        end = min(start + batch_size, trials)\n",
    "\n",
    "        # Generate a batch of random vectors\n",
    "        v1 = np.random.randn(end - start, d)\n",
    "        v2 = np.random.randn(end - start, d)\n",
    "\n",
    "        # Vectorized computation within batch\n",
    "        dot_products = np.einsum('ij,ij->i', v1, v2)\n",
    "        norms_v1 = np.linalg.norm(v1, axis=1)\n",
    "        norms_v2 = np.linalg.norm(v2, axis=1)\n",
    "        cos_angles = dot_products / (norms_v1 * norms_v2)\n",
    "        angles[start:end] = np.degrees(np.arccos(np.clip(cos_angles, -1.0, 1.0)))\n",
    "    \n",
    "    return angles\n",
    "\n",
    "def simulate_vector_angles(n, trials=100_000):\n",
    "    angles = generate_angles(trials, n)\n",
    "    plot_histogram(angles, center=90, title=f'Angle between 2 random vectors (dimensions={n})')\n",
    "\n",
    "simulate_vector_angles(n=3)\n",
    "simulate_vector_angles(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3dc69-7c5e-4c8d-964c-62c732a2f4f9",
   "metadata": {},
   "source": [
    "The number of near-orthagonal vectors in high dimensional spaces is often used as an explanation to why superposition appears in neural networks. \n",
    "\n",
    "The number of embedding dimensions for modern LLMs tends to increase as the models overall size is scaled. For example, for LLaMA 3, the number of dimensions is:\n",
    "- 8B: 4096\n",
    "- 70B: 8192\n",
    "- 405B: 16,384\n",
    "\n",
    "Let's take a look at the distribution of angles between any two random vectors in the embedding space of LLaMa 3's largest model at 405 Billion Parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28235f89-c1f5-458c-8b80-b5b43864a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might take a minute to compute\n",
    "simulate_vector_angles(n=16384)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a45941-5c76-4b44-9c2e-99840257654a",
   "metadata": {},
   "source": [
    "As a bonus, we can go further. As pairs of vectors become increasingly more orthoganol as the number of dimensions increases, so do larger groups of vectors.\n",
    "\n",
    "Let's look at the minimum angle between any two vectors in a group, for different size groups in different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01126098-0f19-4abc-a5d6-7532effbdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_multiple_vector_angles(dimensions, group_size, trials=100_000):\n",
    "    angles = np.empty(trials)\n",
    "    for i in range(trials):\n",
    "        vectors = []\n",
    "        for _ in range(group_size):\n",
    "            vectors.append(np.random.randn(dimensions))\n",
    "\n",
    "        min_angle = 180\n",
    "        for j in range(group_size):\n",
    "            for k in range(j+1, group_size):\n",
    "                min_angle = min(min_angle, find_angle(vectors[j], vectors[k]))\n",
    "                           \n",
    "        angles[i] = min_angle\n",
    "    \n",
    "    plot_histogram(angles, symmetric=False, title=f'Group Vector Minimum Angle. (dimensions={dimensions}, group_size={group_size})')\n",
    "\n",
    "simulate_multiple_vector_angles(100, 2)\n",
    "simulate_multiple_vector_angles(100, 5)\n",
    "simulate_multiple_vector_angles(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a0c77-244e-434e-a3bd-6fec5960678c",
   "metadata": {},
   "source": [
    "Remember these experiments are just to give an intuition about the sparsity of vectors by looking at random vectors. When LLMs are trained, they can essentially 'choose' which vectors to use (associate with a given feature), meaning they can choose a packing which maximises orthagonality between many vectors. \n",
    "\n",
    "You have seen that the average group of vectors becomes more orthogonal to each other as the number of dimensions grows, however also keep in mind that the 'best' group, that which has the maximal orthogonality between each vector, also grows. In fact, it grows much faster than these graphs might indicate - it is known that the maximum number of near-orthogonal vectors scales exponentially with the number of dimensions.\n",
    "\n",
    "The Kabatiansky-Levenshtein Bound gives an asymptotic formula for approximating this value (technically referred to as the maximal size of a spherical code in n dimensions). Note the following KL values are just to illustrate the approximate shape and magnitude of the function, since the KL bound can become innaccurate for theta values close to 90 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d541f8b-b6e0-4387-8ff4-b2543ead23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circle: Points no closer than 45 degrees\n",
    "def plot_circle_with_dots():\n",
    "    fig, ax = plt.subplots()\n",
    "    circle = plt.Circle((0, 0), 1, color='lightgray', fill=False)\n",
    "    ax.add_artist(circle)\n",
    "    \n",
    "    angles_deg = np.arange(0, 360, 45)  # Every 45 degrees\n",
    "    angles_rad = np.deg2rad(angles_deg)\n",
    "    \n",
    "    x = np.cos(angles_rad)\n",
    "    y = np.sin(angles_rad)\n",
    "    \n",
    "    ax.plot(x, y, 'o', color='red', label='45°-spaced dots')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(-1.1, 1.1)\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "    ax.set_title(\"Circle: Dots at ≥45°\")\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Sphere: Greedy algorithm to place points with angular separation ≥ 45°\n",
    "def angular_distance(p1, p2):\n",
    "    dot = np.dot(p1, p2)\n",
    "    return np.arccos(np.clip(dot, -1.0, 1.0))\n",
    "\n",
    "def generate_sphere_points(min_angle_deg):\n",
    "    min_angle_rad = np.deg2rad(min_angle_deg)\n",
    "    points = []\n",
    "\n",
    "    # Generate candidates over the sphere using spherical coordinates\n",
    "    phi_vals = np.linspace(0, np.pi, 100)\n",
    "    theta_vals = np.linspace(0, 2*np.pi, 200)\n",
    "    \n",
    "    for phi in phi_vals:\n",
    "        for theta in theta_vals:\n",
    "            x = np.sin(phi) * np.cos(theta)\n",
    "            y = np.sin(phi) * np.sin(theta)\n",
    "            z = np.cos(phi)\n",
    "            new_point = np.array([x, y, z])\n",
    "\n",
    "            # Check if this point is sufficiently far from all existing points\n",
    "            if all(angular_distance(new_point, p) >= min_angle_rad for p in points):\n",
    "                points.append(new_point)\n",
    "\n",
    "    return np.array(points)\n",
    "\n",
    "def plot_sphere_with_dots():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Draw a sphere\n",
    "    u, v = np.mgrid[0:2*np.pi:100j, 0:np.pi:100j]\n",
    "    xs = np.cos(u) * np.sin(v)\n",
    "    ys = np.sin(u) * np.sin(v)\n",
    "    zs = np.cos(v)\n",
    "    ax.plot_surface(xs, ys, zs, color='lightblue', alpha=0.2)\n",
    "\n",
    "    # Generate and plot dots\n",
    "    sphere_points = generate_sphere_points(45)\n",
    "    ax.scatter(sphere_points[:, 0], sphere_points[:, 1], sphere_points[:, 2], color='red')\n",
    "\n",
    "    ax.set_box_aspect([1,1,1])\n",
    "    ax.set_title(f\"Sphere: Dots at ≥45° (Total: {len(sphere_points)})\")\n",
    "    plt.show()\n",
    "\n",
    "# Run both plots\n",
    "plot_circle_with_dots()\n",
    "plot_sphere_with_dots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899066b4-c5c5-424d-9a88-d1fa17b99fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the exponent (base 10) of the Kabatyanskii-Levenshtein bound\n",
    "def kl_bound(n, theta):\n",
    "    if theta <= 0 or theta >= np.pi / 2:\n",
    "        raise ValueError(\"Theta should be in the range (0, pi/2)\")\n",
    "\n",
    "    sin_theta = np.sin(theta)\n",
    "\n",
    "    # apply KL formula\n",
    "    term1 = (1 + sin_theta) / (2 * sin_theta)\n",
    "    term2 = (1 - sin_theta) / (2 * sin_theta)\n",
    "    log_term1 = np.log2(term1)\n",
    "    log_term2 = np.log2(term2)\n",
    "\n",
    "    kl_bound_value = (term1 * log_term1 - term2 * log_term2)\n",
    "\n",
    "    return round((n * kl_bound_value) / np.log2(10), 2)\n",
    "\n",
    "trial_values = ((16384, 88), (16384, 85), (16384, 80))\n",
    "for n, theta in trial_values:\n",
    "    bound = kl_bound(n, np.radians(theta))\n",
    "    print(f\"The maximum number of vectors with minimum angle separation {theta} in {n} dimensions is approx. 10^{bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608006e-44c2-45d0-9403-fd542e829002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_perpendicular_vectors(ax):\n",
    "    labels = [\"size\", \"red\"]\n",
    "    directions = [(1, 0), (0, 1)]\n",
    "\n",
    "    for (dx, dy), label in zip(directions, labels):\n",
    "        ax.quiver(0, 0, dx, dy, angles='xy', scale_units='xy', scale=1, color='red')\n",
    "        if label == \"size\":\n",
    "            ax.text(dx * 1.3, dy * 1.05, label, color='black', fontsize=16, fontweight='bold', ha='center', va='center')\n",
    "        else:\n",
    "            ax.text(dx * 1.1, dy * 1.1, label, color='black', fontsize=16, fontweight='bold', ha='center', va='center')\n",
    "\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(\"Two Perpendicular Vectors\", fontsize=14, fontweight='bold')\n",
    "    ax.grid(True)\n",
    "\n",
    "def plot_pentagon_vectors(ax):\n",
    "    labels = [\"size\", \"red\", \"vehicle\", \"beauty\", \"animal\"]\n",
    "    n = len(labels)\n",
    "    angle_step = 2 * np.pi / n\n",
    "    radius = 1\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        angle = i * angle_step\n",
    "        dx = radius * np.cos(angle)\n",
    "        dy = radius * np.sin(angle)\n",
    "        ax.quiver(0, 0, dx, dy, angles='xy', scale_units='xy', scale=1, color='red')\n",
    "        ax.text(dx * 1.15, dy * 1.15, label, color='black', fontsize=16, fontweight='bold', ha='center', va='center')\n",
    "\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(\"Five Vectors in a Pentagon Shape\", fontsize=14, fontweight='bold')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plot_perpendicular_vectors(axes[0])\n",
    "plot_pentagon_vectors(axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f8c43-852f-46ea-8875-f235c869e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "def get_pentagon_vectors():\n",
    "    labels = [\"size\", \"red\", \"vehicle\", \"beauty\", \"animal\"]\n",
    "    n = len(labels)\n",
    "    angle_step = 2 * np.pi / n\n",
    "    radius = 1\n",
    "    vectors = []\n",
    "    for i in range(n):\n",
    "        angle = i * angle_step\n",
    "        dx = radius * np.cos(angle)\n",
    "        dy = radius * np.sin(angle)\n",
    "        vectors.append((np.array([dx, dy]), labels[i]))\n",
    "    return vectors\n",
    "\n",
    "def find_valid_combinations(vectors, p):\n",
    "    valid = []\n",
    "    for (i, (v1, l1)), (j, (v2, l2)) in combinations(enumerate(vectors), 2):\n",
    "        M = np.column_stack((v1, v2))\n",
    "        try:\n",
    "            a, b = np.linalg.solve(M, p)\n",
    "            if 0 <= a <= 1 and 0 <= b <= 1:\n",
    "                valid.append((i, j, a, b))\n",
    "        except np.linalg.LinAlgError:\n",
    "            continue\n",
    "    return valid\n",
    "\n",
    "def plot_combination(vectors, i, j, a, b, p):\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "    # Plot all basis vectors, label all\n",
    "    for idx, (v, label) in enumerate(vectors):\n",
    "        color = 'red' if idx in [i, j] else 'lightgray'\n",
    "        ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color=color, width=0.005)\n",
    "        ax.text(v[0] * 1.15, v[1] * 1.15, label,\n",
    "                color='black', fontsize=16, fontweight='bold',\n",
    "                ha='center', va='center')\n",
    "\n",
    "    # Draw the composition path: a * vi then b * vj\n",
    "    vi = vectors[i][0]\n",
    "    vj = vectors[j][0]\n",
    "    p1 = a * vi\n",
    "    ax.quiver(0, 0, p1[0], p1[1], angles='xy', scale_units='xy', scale=1, color='black')\n",
    "    ax.quiver(p1[0], p1[1], p[0] - p1[0], p[1] - p1[1], angles='xy', scale_units='xy', scale=1, color='black')\n",
    "\n",
    "    # Final point\n",
    "    ax.plot(p[0], p[1], 'ro', markersize=8)\n",
    "    ax.set_title(f\"Combination: {vectors[i][1]} + {vectors[j][1]}\", fontsize=14)\n",
    "    \n",
    "    lim = np.max(np.abs([v[0] for v in vectors] + [p])) * 1.5\n",
    "    ax.set_xlim(-lim, lim)\n",
    "    ax.set_ylim(-lim, lim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_vector_combinations(target_point):\n",
    "    vectors = get_pentagon_vectors()\n",
    "    valid_pairs = find_valid_combinations(vectors, target_point)\n",
    "\n",
    "    if not valid_pairs:\n",
    "        print(\"No valid vector pairs found where a, b ∈ [0, 1].\")\n",
    "        return\n",
    "\n",
    "    for i, j, a, b in valid_pairs:\n",
    "        plot_combination(vectors, i, j, a, b, target_point)\n",
    "\n",
    "target = np.array([0.4, 0.3])\n",
    "visualize_vector_combinations(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5244a-b5ba-41ec-9f1f-8a34b0568fc9",
   "metadata": {},
   "source": [
    "As can be observed from the group size graphs, if you increase the group size the average minimum angle between vectors decreases. Therefore, if you want to limit the probability of getting a certain level of similarity between vectors, there is a maximum limit to the group size you can use. In a similar vein, a crucial aspect of the KL bound formula is that the less orthogonality you have between your vectors - in other words the more similar your vectors are - the greater number of vectors you can pack into your space.\n",
    "\n",
    "In deep learning, you want to be able to distinguish different feature vectors, so there is a limit to the allowable similarity between these vectors. If one imagines this allowable similarity as a limited resource, there are then two forces competing for it: a desire to increase the 'group size' (number of simultaneously active features) and a desire to increase the total number of vectors packed into the space (number of total features).\n",
    "\n",
    "This is why the sparsity of features is a useful predictor for the amount of superposition in a neural network. 'sparsity of features' essentially means how often multiple features are active simultaneously as a proportion of the total features (similar to the group size in the experiment). Language can represent a vast array of ideas, but only a very small portion of those are present in a given sentence (or context window). This means the balance between those two forces tips towards the total number of features, and as discussed before in order to be able to increase total features by packing more vectors into the space then you must accept more similar vectors for those features. Greater similarity between feature vectors manifests as superposition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
